{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHJQ8ktE8IpIAoFa4rU0RO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalze1/machine-learning-fraud-detector/blob/Model-Building-and-Training/model_building_and_training_on_google_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "id": "_1PXzwF84G3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "8oSuhVrV4WWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import mlflow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n"
      ],
      "metadata": {
        "id": "DI85AotR4Adz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utz3J1s446kE",
        "outputId": "839b93ec-f4bd-47ef-97a5-8fba518306a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/machine-learning-fraud-detector/new.csv\""
      ],
      "metadata": {
        "id": "vgssFCxZ7RnR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "creditcard_data = pd.read_csv(path)\n",
        "X_creditcard = creditcard_data.drop('Class', axis=1)\n",
        "y_creditcard = creditcard_data['Class']\n",
        "X_train_creditcard, X_test_creditcard, y_train_creditcard, y_test_creditcard = train_test_split(X_creditcard, y_creditcard, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "p-UCETrJ4sjC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
        "text_data = df[['text_column', 'Class']]\n",
        "\n",
        "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
        "                                                    text_data['Class'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_train)  # Optional: Stratified split\n",
        "\n",
        "# Preprocess text data (e.g., tokenization, vectorization)\n",
        "# This step is crucial for Logistic Regression to understand the text sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust vocabulary size as needed\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Scale the target variable (if necessary)\n",
        "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Start MLflow autologging\n",
        "mlflow.autolog()\n",
        "\n",
        "with mlflow.start_run(run_name=\"My Logistic Regression Model (Text Classification) v1\"):\n",
        "    # Define the Logistic Regression model\n",
        "    lr_model = LogisticRegression()\n",
        "\n",
        "    # Train the model\n",
        "    lr_model.fit(X_train_vectorized, y_train_scaled)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = lr_model.predict(X_test_vectorized)\n",
        "    accuracy = accuracy_score(y_test_scaled, y_pred)\n",
        "    f1 = f1_score(y_test_scaled, y_pred)  # Calculate F1-score\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1-score: {f1}\")\n",
        "\n",
        "    # Log additional parameters and metrics to MLflow\n",
        "    mlflow.log_param(\"model_type\", \"logistic_regression\")\n",
        "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "\n",
        "    # Define the Google Drive path\n",
        "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/lr_model1.h5'\n",
        "\n",
        "    # Save the model to Google Drive (adjust the format based on your framework)\n",
        "    import joblib\n",
        "    joblib.dump(lr_model, drive_model_path)\n",
        "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
      ],
      "metadata": {
        "id": "i0sBTT73RYS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
        "text_data = df[['text_column', 'Class']]\n",
        "\n",
        "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
        "                                                    text_data['Class'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_train)  # Optional: Stratified split\n",
        "\n",
        "# Preprocess text data (e.g., tokenization, vectorization)\n",
        "# This step is crucial for Decision Tree to understand the text sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust vocabulary size as needed\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Scale the target variable (if necessary)\n",
        "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Start MLflow autologging\n",
        "mlflow.autolog()\n",
        "\n",
        "with mlflow.start_run(run_name=\"My Decision Tree Model (Text Classification) v1\"):\n",
        "    # Define the Decision Tree model\n",
        "    dt_model = DecisionTreeClassifier()\n",
        "\n",
        "    # Train the model\n",
        "    dt_model.fit(X_train_vectorized, y_train_scaled)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = dt_model.predict(X_test_vectorized)\n",
        "    accuracy = accuracy_score(y_test_scaled, y_pred)\n",
        "    f1 = f1_score(y_test_scaled, y_pred)  # Calculate F1-score\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1-score: {f1}\")\n",
        "\n",
        "    # Log additional parameters and metrics to MLflow\n",
        "    mlflow.log_param(\"model_type\", \"decision_tree\")\n",
        "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "\n",
        "    # Define the Google Drive path\n",
        "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/dt_model1.h5'\n",
        "\n",
        "    # Save the model to Google Drive (adjust the format based on your framework)\n",
        "    import joblib\n",
        "    joblib.dump(dt_model, drive_model_path)\n",
        "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
      ],
      "metadata": {
        "id": "QPXAqCHMRhSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
        "text_data = df[['text_column', 'Class']]\n",
        "\n",
        "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
        "                                                    text_data['Class'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_train)  # Optional: Stratified split\n",
        "\n",
        "# Preprocess text data (e.g., tokenization, vectorization)\n",
        "# This step is crucial for Random Forest to understand the text sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust vocabulary size as needed\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Scale the target variable (if necessary)\n",
        "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Start MLflow autologging\n",
        "mlflow.autolog()\n",
        "\n",
        "with mlflow.start_run(run_name=\"My Random Forest Model (Text Classification) v1\"):\n",
        "    # Define the Random Forest model\n",
        "    rf_model = RandomForestClassifier()\n",
        "\n",
        "    # Train the model\n",
        "    rf_model.fit(X_train_vectorized, y_train_scaled)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = rf_model.predict(X_test_vectorized)\n",
        "    accuracy = accuracy_score(y_test_scaled, y_pred)\n",
        "    f1 = f1_score(y_test_scaled, y_pred)  # Calculate F1-score\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1-score: {f1}\")\n",
        "\n",
        "    # Log additional parameters and metrics to MLflow\n",
        "    mlflow.log_param(\"model_type\", \"random_forest\")\n",
        "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "\n",
        "    # Define the Google Drive path\n",
        "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/rf_model1.h5'\n",
        "\n",
        "    # Save the model to Google Drive (adjust the format based on your framework)\n",
        "    import joblib\n",
        "    joblib.dump(rf_model, drive_model_path)\n",
        "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
      ],
      "metadata": {
        "id": "RUch_93tRr8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
        "text_data = df[['text_column', 'Class']]\n",
        "\n",
        "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
        "                                                    text_data['Class'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_train)  # Optional: Stratified split\n",
        "\n",
        "# Preprocess text data (e.g., tokenization, vectorization)\n",
        "# This step is crucial for Gradient Boosting to understand the text sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust vocabulary size as needed\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Scale the target variable (if necessary)\n",
        "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Start MLflow autologging\n",
        "mlflow.autolog()\n",
        "\n",
        "with mlflow.start_run(run_name=\"My Gradient Boosting Model (Text Classification) v1\"):\n",
        "    # Define the Gradient Boosting model\n",
        "    gb_model = GradientBoostingClassifier()\n",
        "\n",
        "    # Train the model\n",
        "    gb_model.fit(X_train_vectorized, y_train_scaled)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = gb_model.predict(X_test_vectorized)\n",
        "    accuracy = accuracy_score(y_test_scaled, y_pred)\n",
        "    f1 = f1_score(y_test_scaled, y_pred)  # Calculate F1-score\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1-score: {f1}\")\n",
        "\n",
        "    # Log additional parameters and metrics to MLflow\n",
        "    mlflow.log_param(\"model_type\", \"gradient_boosting\")\n",
        "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "\n",
        "    # Define the Google Drive path\n",
        "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/gb_model1.h5'\n",
        "\n",
        "    # Save the model to Google Drive (adjust the format based on your framework)\n",
        "    import joblib\n",
        "    joblib.dump(gb_model, drive_model_path)\n",
        "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
      ],
      "metadata": {
        "id": "6NrkYUwER9iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
        "text_data = df[['text_column', 'Class']]\n",
        "\n",
        "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
        "                                                    text_data['Class'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_train)  # Optional: Stratified split\n",
        "\n",
        "# Preprocess text data (e.g., tokenization, vectorization)\n",
        "# This step is crucial for MLP to understand the text sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust vocabulary size as needed\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Scale the target variable (if necessary)\n",
        "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Start MLflow autologging\n",
        "mlflow.autolog()\n",
        "\n",
        "with mlflow.start_run(run_name=\"My MLP Model (Text Classification) v1\"):\n",
        "    # Define the MLP model\n",
        "    mlp_model = MLPClassifier()\n",
        "\n",
        "    # Train the model\n",
        "    mlp_model.fit(X_train_vectorized, y_train_scaled)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = mlp_model.predict(X_test_vectorized)\n",
        "    accuracy = accuracy_score(y_test_scaled, y_pred)\n",
        "    f1 = f1_score(y_test_scaled, y_pred)  # Calculate F1-score\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1-score: {f1}\")\n",
        "\n",
        "    # Log additional parameters and metrics to MLflow\n",
        "    mlflow.log_param(\"model_type\", \"mlp\")\n",
        "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "\n",
        "    # Define the Google Drive path\n",
        "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/mlp_model1.h5'\n",
        "\n",
        "    # Save the model to Google Drive (adjust the format based on your framework)\n",
        "    import joblib\n",
        "    joblib.dump(mlp_model, drive_model_path)\n",
        "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
      ],
      "metadata": {
        "id": "hjF0JmoRR-Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "\n",
        "# Assuming df is your dataset with the 'Class' column as the target and others as features\n",
        "X = creditcard_data.drop(columns=['Class'])\n",
        "y = creditcard_data['Class']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train_creditcard, X_test_creditcard, y_train_creditcard, y_test_creditcard = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train_creditcard = scaler.fit_transform(X_train_creditcard)\n",
        "X_test_creditcard = scaler.transform(X_test_creditcard)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define a fully connected neural network (FCNN) for tabular data\n",
        "mlflow.autolog()  # Start MLflow autologging\n",
        "\n",
        "with mlflow.start_run(run_name=\"My FCNN Model (credit card based) v1\"):\n",
        "    fcnn_model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train_creditcard.shape[1],)),  # Input layer: 31 features\n",
        "        Dense(32, activation='relu'),  # Hidden layer 1\n",
        "        Dense(16, activation='relu'),  # Hidden layer 2\n",
        "        Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    fcnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    fcnn_model.fit(X_train_creditcard, y_train_creditcard, epochs=10, batch_size=32)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = fcnn_model.evaluate(X_test_creditcard, y_test_creditcard)\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "    # Log additional parameters and metrics to MLflow\n",
        "    mlflow.log_param(\"model_type\", \"fcnn\")\n",
        "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
        "\n",
        "    # Log the model to MLflow\n",
        "    mlflow.keras.log_model(fcnn_model, \"fcnn_model\")\n",
        "\n",
        "    # Get the run ID for reference\n",
        "    run_id = mlflow.active_run().info.run_id\n",
        "    print(f\"run_id: {run_id}\")\n",
        "\n",
        "\n",
        "\n",
        "    # Define the Google Drive path\n",
        "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/fcnn_model1.h5'\n",
        "\n",
        "    # Copy the model to Google Drive\n",
        "    os.system(f'cp {model_path} {drive_model_path}')\n",
        "    print(f\"Model copied to Google Drive at: {drive_model_path}\")\n"
      ],
      "metadata": {
        "id": "XPm-pYd79MBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler  # Adjust scaler based on data type\n",
        "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
        "text_data = df[['text_column', 'Class']]\n",
        "\n",
        "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
        "                                                    text_data['Class'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_train)  # Optional: Stratified split\n",
        "\n",
        "# Preprocess text data (e.g., tokenization, padding, embedding)\n",
        "# This step is crucial for RNNs to understand the text sequences\n",
        "max_len = 100  # Adjust max sequence length based on your data\n",
        "\n",
        "# Tokenize the text sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000)  # Adjust vocabulary size as needed\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Optionally, embed the sequences into dense vectors\n",
        "embedding_dim = 128  # Adjust embedding dimension as needed\n",
        "embedding_matrix = np.random.rand(len(tokenizer.word_index) + 1, embedding_dim)  # Initialize embedding matrix\n",
        "\n",
        "# If you have pre-trained word embeddings, load them here\n",
        "# ...\n",
        "\n",
        "# Scale the target variable (if necessary)\n",
        "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Start MLflow autologging\n",
        "mlflow.autolog()\n",
        "\n",
        "with mlflow.start_run(run_name=\"My RNN Model (Text Classification) v1\"):\n",
        "    # Define the Recurrent Neural Network (RNN) model\n",
        "    rnn_model = Sequential([\n",
        "        Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len),\n",
        "        LSTM(64, return_sequences=True),  # Adjust LSTM units and return_sequences as needed\n",
        "        LSTM(32),  # Adjust LSTM units as needed\n",
        "        Dense(1, activation='sigmoid')  # Adjust output layer for your classification task\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    rnn_model.fit(X_train_padded, y_train_scaled, epochs=10, batch_size=32)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = rnn_model.evaluate(X_test_padded, y_test_scaled)\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "    # Calculate additional metrics (consider F1-score for imbalanced classes)\n",
        "    y_pred = rnn_model.predict_classes(X_test_padded)  # Adjust for classification task\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(f\"F1-score: {f1}\")\n",
        "\n",
        "    # Log additional parameters and metrics to MLflow\n",
        "    mlflow.log_param(\"model_type\", \"rnn\")\n",
        "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"f1_score\", f1)  # Log additional"
      ],
      "metadata": {
        "id": "BYhBtCr7RDrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler  # Adjust scaler based on data type\n",
        "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
        "text_data = df[['text_column', 'Class']]\n",
        "\n",
        "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
        "                                                    text_data['Class'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_train)  # Optional: Stratified split\n",
        "\n",
        "# Preprocess text data (e.g., tokenization, padding, embedding)\n",
        "# This step is crucial for RNNs to understand the text sequences\n",
        "max_len = 100  # Adjust max sequence length based on your data\n",
        "\n",
        "# Tokenize the text sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000)  # Adjust vocabulary size as needed\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Optionally, embed the sequences into dense vectors\n",
        "embedding_dim = 128  # Adjust embedding dimension as needed\n",
        "embedding_matrix = np.random.rand(len(tokenizer.word_index) + 1, embedding_dim)  # Initialize embedding matrix\n",
        "\n",
        "# If you have pre-trained word embeddings, load them here\n",
        "# ...\n",
        "\n",
        "# Scale the target variable (if necessary)\n",
        "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Start MLflow autologging\n",
        "mlflow.autolog()\n",
        "\n",
        "with mlflow.start_run(run_name=\"My LSTM Model (Text Classification) v1\"):\n",
        "    # Define the Long Short-Term Memory (LSTM) model\n",
        "    lstm_model = Sequential([\n",
        "        Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len),\n",
        "        LSTM(64, return_sequences=True),  # Adjust LSTM units and return_sequences as needed\n",
        "        LSTM(32),  # Adjust LSTM units as needed\n",
        "        Dense(1, activation='sigmoid')  # Adjust output layer for your classification task\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    lstm_model.fit(X_train_padded, y_train_scaled, epochs=10, batch_size=32)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = lstm_model.evaluate(X_test_padded, y_test_scaled)\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "    # Calculate additional metrics (consider F1-score for imbalanced classes)\n",
        "    y_pred = lstm_model.predict_classes(X_test_padded)  # Adjust for classification task\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(f\"F1-score: {f1}\")\n",
        "\n",
        "    # Log additional parameters and metrics to MLflow\n",
        "    mlflow.log_param(\"model_type\", \"lstm\")\n",
        "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"f1_score\", f1)  # Log additional\n",
        "\n",
        "    # Define the Google Drive path\n",
        "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/lstm_model1.h5'\n",
        "\n",
        "    # Copy the model to Google Drive\n",
        "    os.system(f'cp {model_path} {drive_model_path}')\n",
        "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
      ],
      "metadata": {
        "id": "d0pTD6I1RRV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}