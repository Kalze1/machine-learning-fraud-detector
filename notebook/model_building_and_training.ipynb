{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "creditcard_data = pd.read_csv('data/creditcard.csv')\n",
    "X_creditcard = creditcard_data.drop('Class', axis=1)\n",
    "y_creditcard = creditcard_data['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_creditcard, y_creditcard, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
    "text_data = df[['text_column', 'Class']]\n",
    "\n",
    "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
    "                                                    text_data['Class'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_train)  # Optional: Stratified split\n",
    "\n",
    "# Preprocess text data (e.g., tokenization, vectorization)\n",
    "# This step is crucial for Logistic Regression to understand the text sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust vocabulary size as needed\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Scale the target variable (if necessary)\n",
    "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Start MLflow autologging\n",
    "mlflow.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"My Logistic Regression Model (Text Classification) v1\"):\n",
    "    # Define the Logistic Regression model\n",
    "    lr_model = LogisticRegression()\n",
    "\n",
    "    # Train the model\n",
    "    lr_model.fit(X_train_vectorized, y_train_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = lr_model.predict(X_test_vectorized)\n",
    "    accuracy = accuracy_score(y_test_scaled, y_pred)\n",
    "    f1 = f1_score(y_test_scaled, y_pred)  # Calculate F1-score\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "\n",
    "    # Log additional parameters and metrics to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"logistic_regression\")\n",
    "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Define the Google Drive path\n",
    "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/lr_model1.h5'\n",
    "\n",
    "    # Save the model to Google Drive (adjust the format based on your framework)\n",
    "    import joblib\n",
    "    joblib.dump(lr_model, drive_model_path)\n",
    "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
    "text_data = df[['text_column', 'Class']]\n",
    "\n",
    "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
    "                                                    text_data['Class'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_train)  # Optional: Stratified split\n",
    "\n",
    "# Preprocess text data (e.g., tokenization, vectorization)\n",
    "# This step is crucial for Decision Tree to understand the text sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust vocabulary size as needed\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Scale the target variable (if necessary)\n",
    "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Start MLflow autologging\n",
    "mlflow.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"My Decision Tree Model (Text Classification) v1\"):\n",
    "    # Define the Decision Tree model\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "\n",
    "    # Train the model\n",
    "    dt_model.fit(X_train_vectorized, y_train_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = dt_model.predict(X_test_vectorized)\n",
    "    accuracy = accuracy_score(y_test_scaled, y_pred)\n",
    "    f1 = f1_score(y_test_scaled, y_pred)  # Calculate F1-score\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "\n",
    "    # Log additional parameters and metrics to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"decision_tree\")\n",
    "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Define the Google Drive path\n",
    "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/dt_model1.h5'\n",
    "\n",
    "    # Save the model to Google Drive (adjust the format based on your framework)\n",
    "    import joblib\n",
    "    joblib.dump(dt_model, drive_model_path)\n",
    "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
    "text_data = df[['text_column', 'Class']]\n",
    "\n",
    "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
    "                                                    text_data['Class'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_train)  # Optional: Stratified split\n",
    "\n",
    "# Preprocess text data (e.g., tokenization, vectorization)\n",
    "# This step is crucial for Random Forest to understand the text sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust vocabulary size as needed\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Scale the target variable (if necessary)\n",
    "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Start MLflow autologging\n",
    "mlflow.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"My Random Forest Model (Text Classification) v1\"):\n",
    "    # Define the Random Forest model\n",
    "    rf_model = RandomForestClassifier()\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_train_vectorized, y_train_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = rf_model.predict(X_test_vectorized)\n",
    "    accuracy = accuracy_score(y_test_scaled, y_pred)\n",
    "    f1 = f1_score(y_test_scaled, y_pred)  # Calculate F1-score\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "\n",
    "    # Log additional parameters and metrics to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"random_forest\")\n",
    "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Define the Google Drive path\n",
    "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/rf_model1.h5'\n",
    "\n",
    "    # Save the model to Google Drive (adjust the format based on your framework)\n",
    "    import joblib\n",
    "    joblib.dump(rf_model, drive_model_path)\n",
    "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
    "text_data = df[['text_column', 'Class']]\n",
    "\n",
    "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
    "                                                    text_data['Class'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_train)  # Optional: Stratified split\n",
    "\n",
    "# Preprocess text data (e.g., tokenization, vectorization)\n",
    "# This step is crucial for Gradient Boosting to understand the text sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust vocabulary size as needed\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Scale the target variable (if necessary)\n",
    "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Start MLflow autologging\n",
    "mlflow.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"My Gradient Boosting Model (Text Classification) v1\"):\n",
    "    # Define the Gradient Boosting model\n",
    "    gb_model = GradientBoostingClassifier()\n",
    "\n",
    "    # Train the model\n",
    "    gb_model.fit(X_train_vectorized, y_train_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = gb_model.predict(X_test_vectorized)\n",
    "    accuracy = accuracy_score(y_test_scaled, y_pred)\n",
    "    f1 = f1_score(y_test_scaled, y_pred)  # Calculate F1-score\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "\n",
    "    # Log additional parameters and metrics to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"gradient_boosting\")\n",
    "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Define the Google Drive path\n",
    "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/gb_model1.h5'\n",
    "\n",
    "    # Save the model to Google Drive (adjust the format based on your framework)\n",
    "    import joblib\n",
    "    joblib.dump(gb_model, drive_model_path)\n",
    "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
    "text_data = df[['text_column', 'Class']]\n",
    "\n",
    "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
    "                                                    text_data['Class'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_train)  # Optional: Stratified split\n",
    "\n",
    "# Preprocess text data (e.g., tokenization, vectorization)\n",
    "# This step is crucial for MLP to understand the text sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust vocabulary size as needed\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Scale the target variable (if necessary)\n",
    "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Start MLflow autologging\n",
    "mlflow.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"My MLP Model (Text Classification) v1\"):\n",
    "    # Define the MLP model\n",
    "    mlp_model = MLPClassifier()\n",
    "\n",
    "    # Train the model\n",
    "    mlp_model.fit(X_train_vectorized, y_train_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = mlp_model.predict(X_test_vectorized)\n",
    "    accuracy = accuracy_score(y_test_scaled, y_pred)\n",
    "    f1 = f1_score(y_test_scaled, y_pred)  # Calculate F1-score\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "\n",
    "    # Log additional parameters and metrics to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"mlp\")\n",
    "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Define the Google Drive path\n",
    "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/mlp_model1.h5'\n",
    "\n",
    "    # Save the model to Google Drive (adjust the format based on your framework)\n",
    "    import joblib\n",
    "    joblib.dump(mlp_model, drive_model_path)\n",
    "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.keras\n",
    "from keras.models import load_model\n",
    "from keras.datasets import mnist  # Example dataset, replace with your actual test data\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "\n",
    "# Path to the downloaded model on your local device\n",
    "model_local_path = 'data/models/fcnn_model1.h5'  # Update this path to the actual file location\n",
    "\n",
    "# Load the saved Keras model from your local path\n",
    "fcnn_model = load_model(model_local_path)\n",
    "print(f\"Model loaded successfully from: {model_local_path}\")\n",
    "\n",
    "# Ensure y_test is a DataFrame (if not, convert)\n",
    "if not isinstance(y_test, pd.DataFrame):\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "\n",
    "# Ensure X_train and X_test are DataFrames (if not, convert)\n",
    "if not isinstance(X_train, pd.DataFrame):\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "if not isinstance(X_test, pd.DataFrame):\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "# Reshape X_test if necessary (consult your dataset structure)\n",
    "# Example: Assuming features are in the second dimension\n",
    "# X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
    "\n",
    "with mlflow.start_run(run_name=\"Loaded FCNN Model (Local MLflow)\"):\n",
    "\n",
    "    # Log the model to MLflow\n",
    "    mlflow.keras.log_model(fcnn_model, \"fcnn_model\")\n",
    "\n",
    "    # Log some parameters\n",
    "    mlflow.log_param(\"model_type\", \"fcnn\")\n",
    "    mlflow.log_param(\"source\", \"local_device\")\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    test_loss, test_accuracy = fcnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Log test metrics\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "    # Optional: Generate predictions and log additional metrics\n",
    "    y_pred = fcnn_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy using sklearn (alternative to Keras evaluation)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Sklearn Accuracy: {accuracy}\")\n",
    "\n",
    "    # Log the sklearn accuracy metric\n",
    "    mlflow.log_metric(\"sklearn_accuracy\", accuracy)\n",
    "\n",
    "    # Optionally, log classification report or other metrics\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(f\"Classification Report: \\n{class_report}\")\n",
    "\n",
    "    # You can log the classification report as an artifact (e.g., save it to a text file)\n",
    "    with open(\"classification_report.txt\", \"w\") as f:\n",
    "        f.write(class_report)\n",
    "    mlflow.log_artifact(\"classification_report.txt\")\n",
    "\n",
    "print(f\"Model and metrics successfully logged to local MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler  # Adjust scaler based on data type\n",
    "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
    "text_data = df[['text_column', 'Class']]\n",
    "\n",
    "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
    "                                                    text_data['Class'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_train)  # Optional: Stratified split\n",
    "\n",
    "# Preprocess text data (e.g., tokenization, padding, embedding)\n",
    "# This step is crucial for RNNs to understand the text sequences\n",
    "max_len = 100  # Adjust max sequence length based on your data\n",
    "\n",
    "# Tokenize the text sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)  # Adjust vocabulary size as needed\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Optionally, embed the sequences into dense vectors\n",
    "embedding_dim = 128  # Adjust embedding dimension as needed\n",
    "embedding_matrix = np.random.rand(len(tokenizer.word_index) + 1, embedding_dim)  # Initialize embedding matrix\n",
    "\n",
    "# If you have pre-trained word embeddings, load them here\n",
    "# ...\n",
    "\n",
    "# Scale the target variable (if necessary)\n",
    "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Start MLflow autologging\n",
    "mlflow.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"My RNN Model (Text Classification) v1\"):\n",
    "    # Define the Recurrent Neural Network (RNN) model\n",
    "    rnn_model = Sequential([\n",
    "        Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len),\n",
    "        LSTM(64, return_sequences=True),  # Adjust LSTM units and return_sequences as needed\n",
    "        LSTM(32),  # Adjust LSTM units as needed\n",
    "        Dense(1, activation='sigmoid')  # Adjust output layer for your classification task\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    rnn_model.fit(X_train_padded, y_train_scaled, epochs=10, batch_size=32)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = rnn_model.evaluate(X_test_padded, y_test_scaled)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Calculate additional metrics (consider F1-score for imbalanced classes)\n",
    "    y_pred = rnn_model.predict_classes(X_test_padded)  # Adjust for classification task\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"F1-score: {f1}\")\n",
    "\n",
    "    # Log additional parameters and metrics to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"rnn\")\n",
    "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Define the Google Drive path\n",
    "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/rnn_model1.h5'\n",
    "\n",
    "    # Save the model to Google Drive (adjust the format based on your framework)\n",
    "    os.system(f'cp {model_path} {drive_model_path}')\n",
    "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler  # Adjust scaler based on data type\n",
    "from sklearn.metrics import accuracy_score, f1_score  # Consider additional metrics\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming df is your dataset with a 'text_column' containing text sequences and a 'Class' column as the target\n",
    "text_data = df[['text_column', 'Class']]\n",
    "\n",
    "# Split data into train and test sets (consider stratified split for imbalanced classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data['text_column'],\n",
    "                                                    text_data['Class'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_train)  # Optional: Stratified split\n",
    "\n",
    "# Preprocess text data (e.g., tokenization, padding, embedding)\n",
    "# This step is crucial for RNNs to understand the text sequences\n",
    "max_len = 100  # Adjust max sequence length based on your data\n",
    "\n",
    "# Tokenize the text sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)  # Adjust vocabulary size as needed\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Optionally, embed the sequences into dense vectors\n",
    "embedding_dim = 128  # Adjust embedding dimension as needed\n",
    "embedding_matrix = np.random.rand(len(tokenizer.word_index) + 1, embedding_dim)  # Initialize embedding matrix\n",
    "\n",
    "# If you have pre-trained word embeddings, load them here\n",
    "# ...\n",
    "\n",
    "# Scale the target variable (if necessary)\n",
    "scaler = MinMaxScaler()  # Adjust scaler based on data type\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Start MLflow autologging\n",
    "mlflow.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"My LSTM Model (Text Classification) v1\"):\n",
    "    # Define the Long Short-Term Memory (LSTM) model\n",
    "    lstm_model = Sequential([\n",
    "        Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len),\n",
    "        LSTM(64, return_sequences=True),  # Adjust LSTM units and return_sequences as needed\n",
    "        LSTM(32),  # Adjust LSTM units as needed\n",
    "        Dense(1, activation='sigmoid')  # Adjust output layer for your classification task\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    lstm_model.fit(X_train_padded, y_train_scaled, epochs=10, batch_size=32)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = lstm_model.evaluate(X_test_padded, y_test_scaled)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Calculate additional metrics (consider F1-score for imbalanced classes)\n",
    "    y_pred = lstm_model.predict_classes(X_test_padded)  # Adjust for classification task\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"F1-score: {f1}\")\n",
    "\n",
    "    # Log additional parameters and metrics to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"lstm\")\n",
    "    mlflow.log_metric(\"final_accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Define the Google Drive path\n",
    "    drive_model_path = '/content/drive/MyDrive/machine-learning-fraud-detector/lstm_model1.h5'\n",
    "\n",
    "    # Save the model to Google Drive (adjust the format based on your framework)\n",
    "    os.system(f'cp {model_path} {drive_model_path}')\n",
    "    print(f\"Model copied to Google Drive at: {drive_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
